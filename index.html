<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>CTKD</title>
  <link rel="icon" type="image/x-icon" href="none">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Curriculum Temperature for <br> Knowledge Distillation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zheng Li</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a> -->
              <span class="author-block">
                <!-- <p>Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, Jian Yang.</p> -->
                <p>Zheng Li<sup>1</sup>, Xiang Li<sup>1*</sup>, Lingfeng Yang<sup>2</sup>,
                  Borui Zhao<sup>3</sup>, Renjie Song<sup>3</sup>, Lei Luo<sup>2</sup>, Jun Li<sup>2</sup>, Jian Yang<sup>1*</sup>
                </p>
              </span></div>
                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <!-- <span class="author-block"> <sup>1</sup>Nankai University, <sup>2</sup>Nanjing University of Science and Technology, <sup>3</sup>Megvii Technology</span> -->
                    <span class="author-block"> <sup>1</sup>NKU, <sup>2</sup>NJUST, <sup>3</sup>MEGVII</span>
                    <br>
                    <span class="author-block"><strong>AAAI 2023</strong></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                    <br>
                    <span class="author-block"><small>zhengli97@mail.nankai.edu.cn</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25236/25008" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  
                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2211.16231" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zhengli97/CTKD" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                  <!-- 中文解读 -->
                  <span class="link-block">
                    <a href="chinese_interpertation.html" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-clone"></i>
                    </span>
                    <span>中文解读</span>
                </a>
                <br><br><br>
                <figure width="20%">
                  <img src="static/images/framework.png" alt="fail" width="90%"">
                  <figcaption >Fig.1 (a) We introduce a
                    learnable temperature module that predicts a suitable temperature τ for distillation. The gradient reversal layer is proposed
                    to reverse the gradient of the temperature module during the backpropagation. (b) Following the easy-to-hard curriculum, we
                    gradually increase the parameter λ, leading to increased learning difficulty w.r.t. temperature for the student.</figcaption>
                </figure>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Most existing distillation methods ignore the flexible role of
            the temperature in the loss function and fix it as a hyperparameter that can be decided by an
            inefficient grid search.
            In general, the tempera-ture controls the discrepancy between
            two distributions and can faithfully determine the difficulty level of the distillation task. 
            Keeping a constant temperature, i.e., a fixed level of task difficulty, is usually suboptimal for a
            growing student during its progressive learning stages.
            In this paper, we propose a simple curriculum-based technique, termed Curriculum 
            Temperature for Knowledge Disti-llation (CTKD), which controls the task difficulty level during 
            the student's learning career through a dynamic and learnable temperature. 
            Specifically, following an easy-to-hard curriculum, we gradually increase the distillation 
            loss w.r.t. the temperature, leading to increased distillation difficulty in an adv-ersarial manner. 
            As an easy-to-use plug-in technique, CTKD can be seamlessly integrated into existing knowledge 
            distillation frameworks and brings general improvements at a negligible additional comp-utation cost. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <p>
            1. We propose to adversarially learn a dynamic temperature hyperparameter during the student’s 
            training process
            with a reversed gradient that aims to maximize the distillation loss between teacher and student.
          </p>
          <P>
            2. We introduce simple and effective curricula which organize the distillation task from 
            easy to hard through a dynamic and learnable temperature parameter.
          </P>
          <P>
            3. Extensive experiment results demonstrate that CTKD is
            a simple yet effective plug-in technique, which consistently improves existing state-of-the-art 
            distillation approaches with a substantial margin on CIFAR-100 and ImageNet.
          </P>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            In this paper,  we propose to adversarially learn a dynamic temperature module that predicts a 
            suit-able temperature value τ for the current training. The temperature module is optimized in the
            oppo-site direction of the student, intending to maximize the distillation loss between the student and tea-cher.
            Inspired by curriculum learning, we further introduce a simple and effective curriculum which organizes the 
            distillation task from easy to hard via directly scaling the distillation loss by magnitude w.r.t. the temperature.
            Two versions of the learnable temperature module are used in our approach, namely Global-T and Instance-T,
             which respectively generate different forms of temperature for the current training, as shown in Fig.2. 
          </p>
          <figure>
            <img src="static/images/global_instance_temp.jpg" alt="fail">
            <figcaption>Fig.2 The illustrations of global and instance-wise temperature modules. B denotes the batch size,
             C denotes the number of classes. τ is the output temperature.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
<div class="container is-small">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content is-small">
        <p>
          This page was built using the Academic Project Page Template which was adopted from the Nerfiesproject page.
          This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
